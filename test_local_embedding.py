#!/usr/bin/env python
"""
æµ‹è¯•æœ¬åœ°åµŒå…¥æ¨¡å‹åŠŸèƒ½
"""
import logging
import torch
from sentence_transformers import SentenceTransformer
from rich.console import Console
from rich.table import Table
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

console = Console()

def test_embedding():
    """æµ‹è¯•æœ¬åœ°åµŒå…¥æ¨¡å‹"""
    try:
        # è®¾ç½®è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        console.print(f"[yellow]ä½¿ç”¨è®¾å¤‡: {device}[/yellow]")
        
        # åˆå§‹åŒ–æ¨¡å‹
        console.print("[yellow]æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹...[/yellow]")
        model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­",
            "è¿™æ˜¯å¦ä¸€ä¸ªæµ‹è¯•å¥å­",
            "è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„å¥å­"
        ]
        
        # ç”Ÿæˆå‘é‡
        console.print("[yellow]æ­£åœ¨ç”Ÿæˆæ–‡æœ¬å‘é‡...[/yellow]")
        embeddings = model.encode(test_texts, convert_to_numpy=True)
        
        # æ˜¾ç¤ºç»“æœ
        table = Table(title="å‘é‡åŒ–ç»“æœ")
        table.add_column("æ–‡æœ¬", style="cyan")
        table.add_column("å‘é‡ç»´åº¦", style="green")
        table.add_column("å‘é‡å‰5ä¸ªå€¼", style="blue")
        
        for text, embedding in zip(test_texts, embeddings):
            table.add_row(
                text,
                str(len(embedding)),
                str(embedding[:5].tolist())
            )
        
        console.print(table)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        console.print("\n[yellow]è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦...[/yellow]")
        similarity_table = Table(title="æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ")
        similarity_table.add_column("æ–‡æœ¬", style="cyan")
        for text in test_texts:
            similarity_table.add_column(text[:10] + "...", style="green")
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        for i, text1 in enumerate(test_texts):
            row = [text1]
            for j, text2 in enumerate(test_texts):
                similarity = np.dot(embeddings[i], embeddings[j]) / (
                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                )
                row.append(f"{similarity:.4f}")
            similarity_table.add_row(*row)
        
        console.print(similarity_table)
        
        # æµ‹è¯•é•¿æ–‡æœ¬
        console.print("\n[yellow]æµ‹è¯•é•¿æ–‡æœ¬å¤„ç†...[/yellow]")
        long_text = "è¿™æ˜¯ä¸€ä¸ªè¾ƒé•¿çš„æµ‹è¯•æ–‡æœ¬ã€‚" * 10
        long_embedding = model.encode(long_text, convert_to_numpy=True)
        console.print(f"é•¿æ–‡æœ¬å‘é‡ç»´åº¦: {len(long_embedding)}")
        
        # æµ‹è¯•æ‰¹å¤„ç†
        console.print("\n[yellow]æµ‹è¯•æ‰¹å¤„ç†åŠŸèƒ½...[/yellow]")
        batch_texts = ["æ‰¹å¤„ç†æµ‹è¯•" + str(i) for i in range(5)]
        batch_embeddings = model.encode(batch_texts, convert_to_numpy=True)
        console.print(f"æ‰¹å¤„ç†æˆåŠŸï¼Œç”Ÿæˆäº† {len(batch_embeddings)} ä¸ªå‘é‡")
        
        console.print("\n[green]âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼[/green]")
        
    except Exception as e:
        console.print(f"[red]âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}[/red]")
        raise

if __name__ == "__main__":
    console.print("[bold blue]ğŸ§  æœ¬åœ°åµŒå…¥æ¨¡å‹æµ‹è¯•[/bold blue]\n")
    test_embedding() 